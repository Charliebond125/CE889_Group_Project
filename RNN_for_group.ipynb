{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEQyVOso1K6ftHUFXq5AD5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Charliebond125/CE889_Group_Project/blob/main/RNN_for_group.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "np.random.seed(42)  # for reproducibility\n",
        "\n",
        "sns.set(style=\"whitegrid\", color_codes=True)\n",
        "sns.set(font_scale=1)\n",
        "\n",
        "pd.set_option('display.max_columns', 60)\n",
        "\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from tensorflow.python.ops.init_ops import RandomNormal\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.layers import SimpleRNN, Flatten, LSTM, Dropout, Dense\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "!pip install tensorflow\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib import ticker\n",
        "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import Markdown, display, Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Flatten, Dropout\n",
        "from tensorflow.keras import layers\n",
        "from keras.callbacks import History\n",
        "from keras import Input\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "sys.path.append(os.path.abspath(os.path.join('..')))\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "from numpy import save\n",
        "from numpy import load\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "9xrDsfFsiUra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "df_store = pd.read_csv('/content/drive/MyDrive/store.csv')\n",
        "\n",
        "df_train = df_train.merge(df_store, on=['Store'], how = 'inner')\n",
        "df_test = df_test.merge(df_store, on=['Store'], how = 'inner')"
      ],
      "metadata": {
        "id": "9Qt7A-GQiVOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conc_datetime_comp():\n",
        "\n",
        "  '''\n",
        "  Function to merge test and train data\n",
        "  specifically for ease of use in processing\n",
        "  and feature engineering\n",
        "\n",
        "  '''\n",
        "\n",
        "  df_train = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "  df_test = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "  df_store = pd.read_csv('/content/drive/MyDrive/store.csv')\n",
        "  df_test['Sales'] = -1\n",
        "  df_train_test = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
        "  df_train_test = df_train_test.merge(df_store, left_on=['Store'], right_on=['Store'],how='inner')\n",
        "\n",
        "\n",
        "  # Create datetime products from the full dataframe\n",
        "  \n",
        "  \n",
        "  df_train_test['Year'] = pd.DatetimeIndex(df_train_test['Date']).year\n",
        "  df_train_test['Quarter'] = pd.DatetimeIndex(df_train_test['Date']).quarter\n",
        "  df_train_test['Month'] = pd.DatetimeIndex(df_train_test['Date']).month\n",
        "  df_train_test['Day'] = pd.DatetimeIndex(df_train_test['Date']).day\n",
        "  df_train_test['WeekofYear'] = pd.DatetimeIndex(df_train_test['Date']).weekofyear\n",
        "\n",
        "  df_train_test['Season'] = np.where(df_train_test[\"Month\"].isin([3,4,5]), \"Spring\",\n",
        "                    np.where(df_train_test[\"Month\"].isin([6,7,8]), \"Summer\",\n",
        "                      np.where(df_train_test[\"Month\"].isin([9,10,11]), \"Autumn\",\n",
        "                          np.where(df_train_test[\"Month\"].isin([12, 1, 2]), \"Winter\", \"None\")))\n",
        "                    )\n",
        "  \n",
        "  ''' \n",
        "  Creating more features to enrich the data\n",
        "  This also means we merge the extra store data\n",
        "  with the current dataset\n",
        "  \n",
        "  '''\n",
        "\n",
        "  df_train_test['CompetitionOpen'] = 12 * (\n",
        "            df_train_test.Year - \n",
        "                  df_train_test.CompetitionOpenSinceYear) + \\\n",
        "                        (df_train_test.Month - \n",
        "                            df_train_test.CompetitionOpenSinceMonth\n",
        "                        )\n",
        "  \n",
        "  df_train_test['CompetitionOpen'] = 12 * (\n",
        "          df_train_test.Year - \n",
        "                df_train_test.CompetitionOpenSinceYear) + \\\n",
        "                      (df_train_test.Month - \n",
        "                          df_train_test.CompetitionOpenSinceMonth\n",
        "                       )\n",
        "\n",
        "  df_train_test['PromoOpen'] = 12 * (\n",
        "      df_train_test.Year - \n",
        "              df_train_test.Promo2SinceYear) + \\\n",
        "                  (df_train_test.WeekofYear\n",
        "         -            df_train_test.Promo2SinceWeek) / 4\n",
        "  \n",
        "   \n",
        "\n",
        "  df_train_test['PromoOpen'] = df_train_test.PromoOpen.apply(\n",
        "      lambda x: x if x > 0 else 0)\n",
        "   \n",
        "\n",
        "  month2str = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May',\n",
        "                  6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep',\n",
        "            10:'Oct', 11:'Nov', 12: 'Dec'}\n",
        "\n",
        "\n",
        "  df_train_test['month2str'] = df_train_test.Month.map(month2str)\n",
        "  df_train_test.loc[df_train_test.PromoInterval == 0, 'PromoInterval'] = ''\n",
        "  df_train_test['IsPromoMonth'] = 0\n",
        "  for interval in df_train_test.PromoInterval.unique():\n",
        "    interval = str(interval)\n",
        "    if interval != '':\n",
        "      for month in interval.split(','):\n",
        "        df_train_test.loc[\n",
        "            (df_train_test.month2str == month) % (df_train_test.PromoInterval == interval), 'IsPromoMonth'] = 1\n",
        "\n",
        "\n",
        "  \n",
        "  return df_train_test\n",
        "\n",
        "df_train_test = conc_datetime_comp()"
      ],
      "metadata": {
        "id": "vgv3F7j5isES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_test_data(df):\n",
        "  '''\n",
        "  This is possible as our test data has 0 in sales\n",
        "  therefore we only select the data from our test set\n",
        "\n",
        "  '''\n",
        "  \n",
        "  df_train = df.loc[df['Sales'] != -1]\n",
        "  df_test = df.loc[df['Sales'] == -1]\n",
        "  \n",
        "  return df_train, df_test\n",
        "df_train, df_test = extract_test_data(df_train_test)"
      ],
      "metadata": {
        "id": "YFZ1DTNMivSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(use_text_columns = True):\n",
        "  \n",
        "  ''' \n",
        "  Creating function which will generate our train and test data, \n",
        "  clean it, and return processed with correct values for identified features\n",
        "  \n",
        "  ### Note: When importing the train, test and store data ###\n",
        "  ###       ensure it is pointed at the correct location  ###\n",
        "\n",
        "  '''\n",
        "\n",
        "  cols_num = [\"Sales\", \"DayOfWeek\", \"Open\", \"Promo\", \"SchoolHoliday\", \"CompetitionDistance\",\n",
        "                \"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\", \"Promo2\",\n",
        "                \"Promo2SinceWeek\", \"Promo2SinceYear\", \"Customers\", \"Year\", \"Month\", \"Day\",\n",
        "                \"CompetitionOpen\", \"PromoOpen\", \"IsPromoMonth\", \"Store\"]\n",
        "\n",
        "  cols_text = [\"StateHoliday\", \"StoreType\", \"Assortment\"]\n",
        "  df_train = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "  \n",
        "  df_extra = pd.read_csv('/content/drive/MyDrive/store.csv')\n",
        "      \n",
        "  len_train_data = len(df_train)\n",
        "\n",
        "  df_test = pd.read_csv('/content/drive/MyDrive/test.csv')\n",
        "\n",
        "  # Setting null values of column Open in test dataset\n",
        "  df_test.loc[df_test['DayOfWeek'] != 7, 'Open'] = 1\n",
        "  df_test.loc[df_test['DayOfWeek'] == 7, 'Open'] = 0\n",
        "\n",
        "  # Merge train and test dataset\n",
        "  all_data = pd.concat([df_train, df_test], ignore_index=True)\n",
        "\n",
        "  df_extra = pd.read_csv('/content/drive/MyDrive/store.csv')\n",
        "  df_full = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
        "\n",
        "  # Merge extra information about stores\n",
        "  all_data = df_full.merge(df_extra, left_on=['Store'], right_on=['Store'], how='left')\n",
        "\n",
        "  # Separate date in Year, Month and Day\n",
        "  all_data.loc[all_data['StateHoliday'] == 0, 'StateHoliday'] = 'd'\n",
        "  all_data['Year'] = pd.DatetimeIndex(all_data['Date']).year\n",
        "  all_data['Month'] = pd.DatetimeIndex(all_data['Date']).month\n",
        "  all_data['Day'] = pd.DatetimeIndex(all_data['Date']).day\n",
        "  all_data['WeekOfYear'] = pd.DatetimeIndex(all_data['Date']).weekofyear\n",
        "\n",
        "  # Calculate competition open in months\n",
        "  all_data['CompetitionOpen'] = 12 * (\n",
        "      all_data.Year - all_data.CompetitionOpenSinceYear) + \\\n",
        "          (all_data.Month - all_data.CompetitionOpenSinceMonth)\n",
        "\n",
        "  # Calculate promo open time in months\n",
        "  all_data['PromoOpen'] = 12 * (\n",
        "      all_data.Year - all_data.Promo2SinceYear) + \\\n",
        "          (all_data.WeekOfYear - all_data.Promo2SinceWeek) / 4.0\n",
        "  \n",
        "  all_data['PromoOpen'] = all_data.PromoOpen.apply(\n",
        "      lambda x: x if x > 0 else 0)\n",
        "  \n",
        "  all_data.loc[all_data.Promo2SinceYear == 0, 'PromoOpen'] = 0\n",
        "  \n",
        "  # Transform month interval in a boolean column \n",
        "  month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun',\n",
        "                7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
        "  all_data['monthStr'] = all_data.Month.map(month2str)\n",
        "  all_data.loc[all_data.PromoInterval == 0, 'PromoInterval'] = ''\n",
        "  all_data['IsPromoMonth'] = 0\n",
        "  for interval in all_data.PromoInterval.unique():\n",
        "      interval = str(interval)\n",
        "      if interval != '':\n",
        "          for month in interval.split(','):\n",
        "              all_data.loc[(all_data.monthStr == month) & (all_data.PromoInterval == interval), 'IsPromoMonth'] = 1\n",
        "\n",
        "  data_numeric = all_data[cols_num]\n",
        "  \n",
        "  # Fill NAN values\n",
        "  # Only column CompetitionDistance is fill NaN with a median value\n",
        "  data_numeric['CompetitionDistance'].fillna(data_numeric['CompetitionDistance'].median(), inplace = True)\n",
        "\n",
        "  # Other values is fill with zero\n",
        "  data_numeric.fillna(0, inplace = True)\n",
        "\n",
        "  if (use_text_columns):\n",
        "      data_text = all_data[cols_text]\n",
        "      data_text = pd.get_dummies(data_text, dummy_na=False)\n",
        "\n",
        "      complete_data = pd.concat([data_numeric, data_text], axis = 1)\n",
        "\n",
        "      df_train = complete_data.iloc[:len_train_data,:]\n",
        "      df_test = complete_data.iloc[len_train_data:,:]\n",
        "  else:\n",
        "      df_train = data_numeric.iloc[:len_train_data,:]\n",
        "      df_test = data_numeric.iloc[len_train_data:,:]\n",
        "\n",
        "  return df_train, df_test"
      ],
      "metadata": {
        "id": "dDUmJK4VixJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insepcting closed stores and removing if any\n",
        "\n",
        "'''\n",
        "As can be seen in the below table, we have stores which were closed when it was not a state holiday\n",
        "\n",
        "These will be removed as they may give us outliers and affect the data\n",
        "\n",
        "'''\n",
        "df_train_closed = df_train[(df_train.Open ==0) & (df_train.Sales == 0)\n",
        "                  & (df_train.StateHoliday_0 ==0 )\n",
        "                  & (df_train.SchoolHoliday ==0 )].head()\n",
        "\n",
        "df_train_closed.head()"
      ],
      "metadata": {
        "id": "UXrtl5A-i20x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.drop(df_train[\n",
        "    (df_train.Open ==0) \n",
        "    & (df_train.Sales == 0) & (df_train.SchoolHoliday ==0 )\n",
        "    & (df_train.StateHoliday_0 == 0) & (df_train.StateHoliday_a == 0) \n",
        "    & (df_train.StateHoliday_b==0) & (df_train.StateHoliday_c==0) \n",
        "    & (df_train.StateHoliday_d==0)].index)"
      ],
      "metadata": {
        "id": "JlE5I0cwi8dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.drop(['StateHoliday_a', 'StateHoliday_b', 'StateHoliday_c', 'StateHoliday_c', 'StateHoliday_d'], axis=1)"
      ],
      "metadata": {
        "id": "EU6KBiHii_oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df_test.drop(['StateHoliday_a', 'StateHoliday_b', 'StateHoliday_c', 'StateHoliday_c', 'StateHoliday_d'], axis=1)"
      ],
      "metadata": {
        "id": "08uLkaJ9jBGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaler_function_train(scaler_X, scaler_y):\n",
        "  from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "  robust = RobustScaler()\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  X_train = df_train.drop(['Sales'], axis=1); ''' feature variable '''\n",
        "  y_train = np.array(df_train[\"Sales\"]).reshape((len(X_train), 1)); ''' y target '''\n",
        "  X_train = robust.fit_transform(X_train)\n",
        "  y_train = robust.fit_transform(y_train)\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  y_train = scaler.fit_transform(y_train)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "  return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = scaler_function_train(df_train, df_test)"
      ],
      "metadata": {
        "id": "R66XLwJCjDvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import load\n",
        "\n",
        "X_train = load('/content/drive/MyDrive/Scaled_data/X_train.npy')\n",
        "X_test = load('/content/drive/MyDrive/Scaled_data/X_test.npy')\n",
        "y_train = load('/content/drive/MyDrive/Scaled_data/y_train.npy')\n",
        "y_test = load('/content/drive/MyDrive/Scaled_data/y_test.npy')"
      ],
      "metadata": {
        "id": "8ao57bbHjJqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmspe_val(y_true, y_pred):\n",
        "    '''\n",
        "    RMSPE calculus to validate evaluation metric about the model\n",
        "    '''\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true), axis=0))[0]"
      ],
      "metadata": {
        "id": "PaRt9jQuiVUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmspe(y_true, y_pred):\n",
        "    '''\n",
        "    RMSPE calculus to use during training phase\n",
        "    '''\n",
        "    return K.sqrt(K.mean(K.square((y_true - y_pred) / y_true), axis=-1))"
      ],
      "metadata": {
        "id": "TS1zepANiVZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    '''\n",
        "    RMSE calculus to use during training phase\n",
        "    '''\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
      ],
      "metadata": {
        "id": "S4qpj2c7iVen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_timestep(input_x, window_size):\n",
        "  \n",
        "      nrows = input_x.shape[0] - window_size + 1\n",
        "      p,q = input_x.shape\n",
        "      m,n = input_x.strides\n",
        "      strided = np.lib.stride_tricks.as_strided\n",
        "      output_X = strided(input_x,shape=(nrows,window_size,q),strides=(m,m,n))\n",
        "  \n",
        "      return output_X"
      ],
      "metadata": {
        "id": "CMqMHpi4iPmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "Reshaping an array to give it (n, n, n)\n",
        "is necessary before feeding into a RNN\n",
        "\n",
        "window_size --> passes in the current iteration, as well as n previous layers.\n",
        "\n",
        "window_size is used for noisy or erroneous data, the window is used to mitigate\n",
        "\n",
        "window_size can be defined from (1-->n)\n",
        "\n",
        "'''\n",
        "\n",
        "# create_timestep(input, window_size)"
      ],
      "metadata": {
        "id": "15uWxaKOnTr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RNN_model():\n",
        "  ''' \n",
        "  RNN model for group project \n",
        "  Timestep  = Processing of the input through the recurrent unit\n",
        "\n",
        "  Input Layer --> Hidden/Recurrent Layer --> Output Layer\n",
        "\n",
        "  Each section of the hidden layer contains a recurrent unit, which processes information\n",
        "  for n timesteps. Passes through a hidden state and an input for each timestep using an\n",
        "  activation function.\n",
        "\n",
        "  Each Recurrent unit contains three parameters, which must be specified before feeding \n",
        "  into the model. RNN uses folding in time to train the weights of the network. This is called\n",
        "  Back Propogation through Time i.e. BPTT.\n",
        "  \n",
        "  '''\n",
        "\n",
        "  #813767, 1, 26\n",
        "  initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
        "\n",
        "  model= Sequential(name=\"First-RNN-Model\") # Model\n",
        "\n",
        "  model.add(Input(shape=(4, 26), name= \"Input-Layer\")) # Input layer\n",
        "\n",
        "  ''' First hidden layer ''' \n",
        "  model.add(layers.SimpleRNN(units=256, input_dim=X_train.shape[1], \n",
        "                             activation=\"tanh\", return_sequences=True,\n",
        "                             kernel_initializer=initializer,\n",
        "                             name='First-Hidden-RNN'))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  ''' Second hidden layer '''\n",
        "  model.add(layers.SimpleRNN(units=512, input_dim=X_train.shape[1], \n",
        "                              activation=\"tanh\", return_sequences=True,\n",
        "                                kernel_initializer=initializer,\n",
        "                                  name='Second-Hidden_RNN'))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  ''' third hidden layer '''\n",
        "  model.add(layers.SimpleRNN(units=512, activation=\"relu\",\n",
        "                               return_sequences=True,\n",
        "                                 kernel_initializer=initializer,\n",
        "                                   name='Third-Hidden-RNN'))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  ''' fourth hidden layer '''\n",
        "  model.add(layers.SimpleRNN(units=256, activation=\"relu\",\n",
        "                               return_sequences=True,\n",
        "                                 kernel_initializer=initializer, \n",
        "                                   name='Fourth-Recurrent-Layer-3'))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  ''' fifth hidden layer '''\n",
        "  model.add(layers.Dense(units=16, activation=\"tanh\"))\n",
        "\n",
        "  ''' output layer '''\n",
        "  model.add(layers.Dense(units=1, activation='linear'))\n",
        "  \n",
        "  adam = Adam(lr=1e-7)\n",
        "\n",
        "  model.compile(optimizer=adam,\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[rmse, rmspe])\n",
        "  \n",
        "  return model\n",
        "\n",
        "rnn_model = RNN_model()\n",
        "rnn_model.summary()"
      ],
      "metadata": {
        "id": "CBd12IL_0zUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building model...\")\n",
        "\n",
        "filepath = 'RNN_model_weights.hdf5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, \n",
        "                                    monitor='val_loss',\n",
        "                                      verbose=0,\n",
        "                                          save_best_only=True,\n",
        "                                             mode='max'\n",
        "                                                        )\n",
        "\n",
        "earlystopping = EarlyStopping(monitor='val_loss',\n",
        "                                    patience = 10,\n",
        "                                        verbose=0,\n",
        "                                              mode='max',\n",
        "                                                baseline=0.6\n",
        "                                                        )\n"
      ],
      "metadata": {
        "id": "gOpFGYxqjgmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, \n",
        "                        y_train, \n",
        "                          validation_split=0.2,\n",
        "                            batch_size=batch_size, \n",
        "                              epochs = nb_epoch,\n",
        "                    callbacks=earlystopping, shuffle=True\n",
        "                          )"
      ],
      "metadata": {
        "id": "aDwUESBSjp0U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}